{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-in-Arabic_Lab4-RankedRetrieval.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heba14101998/IR-in-Arabic/blob/master/Summer2021/labs/day4/IR_in_Arabic_Lab4_RankedRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **IR in Arabic** - Summer 2021 lab day4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. It demonstrates how can we perform ranked retrieval and evaluate the output.\n",
        "\n",
        "The **learning outcomes** of this notebook are:\n",
        "\n",
        "\n",
        "*   Retrieval using a vector space model and a BM25 model.\n",
        "*   Evaluating the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "### **Setup**\n",
        "We will first install Pyterrier as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU"
      },
      "source": [
        "#install the Pyterrier framework\n",
        "!pip install python-terrier -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIzygfXzAILT"
      },
      "source": [
        "The next step is to initialize PyTerrier. This is performed using PyTerrier's init() method. The init() method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent init() from being called more than once by checking started()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSW5eI-TQGX"
      },
      "source": [
        "Another library that we need for this lab is Arabic-Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENifjmGeTf6Q"
      },
      "source": [
        "#install the Arabic stop words library\n",
        "!pip install Arabic-Stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnwlwDLJT48p"
      },
      "source": [
        "We will import all the python libraries needed for this lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os60lty5UD-s"
      },
      "source": [
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import numpy as np\n",
        "import re\n",
        "from snowballstemmer import stemmer\n",
        "from tqdm import tqdm\n",
        "import arabicstopwords.arabicstopwords as stp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkxBHwFE8LH6"
      },
      "source": [
        "We will prepare our helper functions for removing stop words, normalize, and stemming which we will use to process our queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KWReIAF8Rxz"
      },
      "source": [
        "#removing Stop Words function\n",
        "def remove_stopWords(sentence):\n",
        "    terms=[]\n",
        "    stopWords= set(stp.stopwords_list())\n",
        "    for term in sentence.split() :\n",
        "        if term not in stopWords :\n",
        "           terms.append(term)\n",
        "    return \" \".join(terms)\n",
        "\n",
        "#a function to normalize the tweets\n",
        "def normalize(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    return(text)\n",
        "\n",
        "#define the stemming function\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "def stem(sentence):\n",
        "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ170aUr7b2D"
      },
      "source": [
        "We will use our indexed **EveTAR** dataset. The index is uploaded in our Github repository so we will access it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkHkRSGz7sfF"
      },
      "source": [
        "# %rm -rf IR-in-Arabic\n",
        "# %rm -rf evetarIndex\n",
        "!git clone https://github.com/telsayed/IR-in-Arabic.git\n",
        "!unzip IR-in-Arabic/Summer2021/data/EveTAR/evetarIndex.zip -d evetarIndex\n",
        "!ls evetarIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkznRg8-VXx"
      },
      "source": [
        "Next, we will load our index. We jus need the data.properties file to load our index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuqjM544-ZKx"
      },
      "source": [
        "#we will load the index\n",
        "index_ref = pt.IndexRef.of(\"./evetarIndex/data.properties\")\n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgIpOhpCL_QO"
      },
      "source": [
        "### **Vector space model retrieval**\n",
        "We will use BatchRetrieve Pyterrier class for retrieval and TF-IDF as the weighting model. You can check the weighting models supported by PyTerrier [here](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-N8p6NpMOha"
      },
      "source": [
        "#set up our retieval model by specifing TF_IDF as wmodel and limiting the number of retrieved results for each query top 10 documents\n",
        "tfidf_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"TF_IDF\"},num_results=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7V5-Ih4M36J"
      },
      "source": [
        "You can query using a simple string. **Note:** you need to preprocess the query using the same processing steps you performed before indexing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O371L3r0NBw8"
      },
      "source": [
        "#we need to process the query also as we did for documents\n",
        "def preprocess(sentence):\n",
        "  # apply preprocessing steps on the given sentence\n",
        "  # students ToDo .....\n",
        "  # here you should write your code\n",
        "  return sentence\n",
        "\n",
        "query=\"العربية\"\n",
        "query = preprocess(query)\n",
        "#we will call the search function using our retrieval model we set up above\n",
        "results=tfidf_retr.search(query)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRBeLsRvC1Dv"
      },
      "source": [
        "Let's check the tweets retrieved by getting the tweets text from our collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-bw0opJC77n"
      },
      "source": [
        "dataset_links=[\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-01.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-02.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-03.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-04.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-05.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-06.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-07.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-08.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-09.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-10.txt\"]\n",
        "\n",
        "full_data=pd.DataFrame()\n",
        "for i in tqdm(range(len(dataset_links))):\n",
        "    tweets=pd.read_csv(dataset_links[i], sep='\\t')\n",
        "    full_data=pd.concat([full_data,tweets],ignore_index=True)\n",
        "full_data.reset_index(inplace=True,drop=True)\n",
        "#the docno will be our tweetID\n",
        "full_data[\"docno\"]=full_data[\"tweetID\"].astype(str)\n",
        "#select tweet text for the tweets retrieved only\n",
        "full_data[full_data[\"docno\"].isin(results[\"docno\"].tolist())]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGrQzoKfFmt-"
      },
      "source": [
        "Let's try another query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl2OFswZCQ9u"
      },
      "source": [
        "#we need to process the term also as we did for documents\n",
        "query=\"أمريكا دولار\"\n",
        "#preprocess\n",
        "query = preprocess(query)\n",
        "#we will call the search function using our retrieval model we set up above\n",
        "results=tfidf_retr.search(query)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oghHX0_pzDSk"
      },
      "source": [
        "Let's check our results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Yue4KyDzlv"
      },
      "source": [
        "# retrieve the tweets text for the retrieved tweets just to check our results\n",
        "full_data[full_data[\"docno\"].isin(results[\"docno\"].tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIL0VhkxXi6z"
      },
      "source": [
        "We will load queries (topics titles) that are already defined and released with EveTAR dataset and process using the same processing steps we did when we indexed EveTAR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAjn8zFGW4x0"
      },
      "source": [
        "#read the topics file from Github and use the titles as queries\n",
        "topics=pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/topics.txt\", sep='\\t',names=['data'])\n",
        "queries=[]\n",
        "qid=[]\n",
        "#we will get the queries and their ids from the topics file\n",
        "for i in range(len(topics)):\n",
        "    splitted=topics[\"data\"][i:i+1][i].split(\" \")\n",
        "    if splitted[0]==\"<title>\":\n",
        "       queries.append(' '.join(splitted[1:]))\n",
        "    if splitted[0]==\"<num>\":\n",
        "       qid.append(splitted[2])\n",
        "\n",
        "queriesDF=pd.DataFrame()\n",
        "queriesDF[\"qid\"]=qid\n",
        "queriesDF[\"raw_query\"]=queries\n",
        "#remove the stopwords from queries, do normalization, and apply stemming\n",
        "queriesDF[\"query\"]=queriesDF[\"raw_query\"].apply(preprocess)\n",
        "\n",
        "queriesDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9P61FAxNehR"
      },
      "source": [
        "We can retrieve the relevant documents to a set of queries using the **transform** function. We will use the set of queries we prepared earlier. The input should be a dataframe containing the **qid** and **query**. The function will return the same dataframe but with an extra 4 columns **docid**, **docno**, **rank**, and **score**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5He7zRGNpYP"
      },
      "source": [
        "#the queries dataframe should have qid and query columns\n",
        "tfidf_res=tfidf_retr.transform(queriesDF)\n",
        "tfidf_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su6rpeNrOJH7"
      },
      "source": [
        "### **BM25**\n",
        "\n",
        "We will initialize our BM25 retrieval model by using the BatchRetrieve class and setting the weighting model to BM25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkSaFH67OvFf"
      },
      "source": [
        "#specify BM25 as wmodel\n",
        "bm25_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"},num_results=10)\n",
        "#the queries dataframe should have qid and query columns\n",
        "bm25_res=bm25_retr.transform(queriesDF)\n",
        "bm25_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdUzSE0mPLTP"
      },
      "source": [
        "### **Evaluating our results**\n",
        "To evaluate the results we need qrels (relevance judgements). The qrels should be in [TREC format](https://trec.nist.gov/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28ejDvJRbx5"
      },
      "source": [
        "qrels=pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/qrels.txt\", sep='\\t',names=['qid','Q0','docno','label'])\n",
        "qrels['docno']=qrels['docno'].astype(str)\n",
        "# qrels are in TREC format\n",
        "qrels = qrels[qrels[\"docno\"].isin(full_data[\"docno\"].tolist())] # to choose qrels for the chosen 50k documents\n",
        "qrels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozsa0BVoRo4t"
      },
      "source": [
        "Let's see an example of a dataset with graded relevance judgement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXnbTiNGIW45"
      },
      "source": [
        "#check the following dataset available by PyTerrier. The relevance is graded\n",
        "pt.get_dataset(\"trec-robust-2004\").get_qrels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVvjj4EDI6jR"
      },
      "source": [
        "#check the unique labels\n",
        "pt.get_dataset(\"trec-robust-2004\").get_qrels()['label'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfBwU7oXyPLB"
      },
      "source": [
        "To evaluate our results we will use Pyterrier Utils.evaluate function. This function take the results and the qrels dataframe containing three columns which are **qid, docno, label.**\n",
        "\n",
        "You can add the following parameters:\n",
        "\n",
        "\n",
        "*   **metrics**: default = [\"map\", ndcg\"], select the evaluation metrics\n",
        "\n",
        "*   **perquery**: default = False, select whether to show the mean of the metrics or the metrics for each queryList item\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUpw9cssyoLq"
      },
      "source": [
        "# Here, we are evaluating TF_IDF retrieval model\n",
        "eval = pt.Utils.evaluate(tfidf_res,qrels[['qid','docno','label']],metrics=[\"map\",\"recall\",\"P\"])\n",
        "eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdMp3pb0uK2"
      },
      "source": [
        "# Here, we are evaluating BM25 retrieval model\n",
        "eval = pt.Utils.evaluate(bm25_res,qrels[['qid','docno','label']],metrics=[\"map\",\"recall\",\"P\"])\n",
        "eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfj5Swv2URm"
      },
      "source": [
        "# Here, we are evaluating bm25 retreival model BUT with activating perquery flag\n",
        "eval = pt.Utils.evaluate(bm25_res,qrels[['qid','docno','label']],metrics=[\"map\",\"recall\",\"P\"],perquery=True)\n",
        "eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbEocrBp7sTm"
      },
      "source": [
        "### **Exercise 1**\n",
        "Select three queries from our 50 queries and retrieve the top 25 relevant tweets for those queries using both the TF-IDF and the BM25 retrieval models. Evaluate your results in terms of precision only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFMPfu18nP3"
      },
      "source": [
        "# write your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6590-CeSgSOr"
      },
      "source": [
        "### **Exercise 2**\n",
        "Given the following queries:\n",
        "\n",
        "['E14' 'E48' 'E36' 'E58' 'E19' 'E63' 'E30' 'E27' 'E39' 'E21']\n",
        "\n",
        "1. Retreive the top 1000 relevant documents using BM25.\n",
        "2. Retrieve the text for both queries and documents and make them into one dataframe.\n",
        "3. Save the resulted dataframe into a text file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgRvwa2dUdBb"
      },
      "source": [
        "selected_queries = ['E14','E48', 'E36', 'E58', 'E19', 'E63', 'E30', 'E27', 'E39', 'E21']\n",
        "# write your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "### **References**\n",
        "\n",
        "\n",
        "* [PyTerrier  retrieval and evaluation notebook](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/retrieval_and_evaluation.ipynb).\n",
        "*   [PyTerrier documentation.](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/)\n",
        "\n"
      ]
    }
  ]
}